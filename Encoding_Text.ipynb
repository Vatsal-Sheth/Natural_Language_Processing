{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "###Encoding Text\n",
        "As we know, machine learning models and neural networks don't take raw text data as an input. This means we must somehow encode our textual data to numeric values that our models can understand. There are many different ways of doing this, and we will look at a few examples below.\n",
        "Before we get into the different encoding/preprocessing methods, let's understand the information we can get from textual data by looking at the following two movie reviews:\n",
        "\"I thought the movie was going to be bad, but it was actually amazing!\"\n",
        "\"I thought the movie was going to be amazing, but it was actually bad!\"\n",
        "Although these two sentences are very similar, we know that they have very different meanings. This is because of the ordering of words, a very important property of textual data. Now keep that in mind while we consider some different ways of encoding our textual data."
      ],
      "metadata": {
        "id": "KYmRpohbj0Uh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Bag of Words\n",
        "The first and simplest way to encode our data is to use something called bag of words. This is a pretty easy technique where each word in a sentence is encoded with an integer and thrown into a collection that does not maintain the order of the words but does keep track of the frequency. Have a look at the Python function below that encodes a string of text into bag of words."
      ],
      "metadata": {
        "id": "flgjgcOfkKc2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AwEIKbvNi-KV",
        "outputId": "2c65b8fd-831d-4cd8-a465-22c097eefe95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{1: 2, 2: 3, 3: 3, 4: 3, 5: 1, 6: 1, 7: 1, 8: 1, 9: 1}\n",
            "{'this': 1, 'is': 2, 'a': 3, 'test': 4, 'to': 5, 'see': 6, 'if': 7, 'will': 8, 'work': 9}\n"
          ]
        }
      ],
      "source": [
        "vocab = {}  # maps word to integer representing it\n",
        "word_encoding = 1\n",
        "def bag_of_words(text):\n",
        "  global word_encoding\n",
        "\n",
        "  words = text.lower().split(\" \")  # create a list of all of the words in the text, well assume there is no grammar in our text for this example\n",
        "  bag = {}  # stores all of the encodings and their frequency\n",
        "\n",
        "  for word in words:\n",
        "    if word in vocab:\n",
        "      encoding = vocab[word]  # get encoding from vocab\n",
        "    else:\n",
        "      vocab[word] = word_encoding\n",
        "      encoding = word_encoding\n",
        "      word_encoding += 1\n",
        "\n",
        "    if encoding in bag:\n",
        "      bag[encoding] += 1\n",
        "    else:\n",
        "      bag[encoding] = 1\n",
        "\n",
        "  return bag\n",
        "\n",
        "text = \"this is a test to see if this test will work is is test a a\"\n",
        "bag = bag_of_words(text)\n",
        "print(bag)\n",
        "print(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "positive_review = \"I thought the movie was going to be bad but it was actually amazing\"\n",
        "negative_review = \"I thought the movie was going to be amazing but it was actually bad\"\n",
        "\n",
        "pos_bag = bag_of_words(positive_review)\n",
        "neg_bag = bag_of_words(negative_review)\n",
        "\n",
        "print(\"Positive:\", pos_bag)\n",
        "print(\"Negative:\", neg_bag)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1rg05KyfjfI1",
        "outputId": "3ecc429e-5a79-4320-e70e-bed302525345"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Positive: {10: 1, 11: 1, 12: 1, 13: 1, 14: 2, 15: 1, 5: 1, 16: 1, 17: 1, 18: 1, 19: 1, 20: 1, 21: 1}\n",
            "Negative: {10: 1, 11: 1, 12: 1, 13: 1, 14: 2, 15: 1, 5: 1, 16: 1, 21: 1, 18: 1, 19: 1, 20: 1, 17: 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that even though these sentences have a very different meaning they are encoded exaclty the same way. Obviously, this isn't going to fly. Let's look at some other methods.\n",
        "\n"
      ],
      "metadata": {
        "id": "tmgQPMGjkXiS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Integer Encoding\n",
        "The next technique we will look at is called integer encoding. This involves representing each word or character in a sentence as a unique integer and maintaining the order of these words. This should hopefully fix the problem we saw before were we lost the order of words.\n",
        "To do this, we first need to create a mapping between each unique word and an integer. We can do this by creating a dictionary that maps each word to a unique integer. Then, when we want to encode a sentence, we simply replace each word with its corresponding integer."
      ],
      "metadata": {
        "id": "6PqCq-5qkbnN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = {}\n",
        "word_encoding = 1\n",
        "def one_hot_encoding(text):\n",
        "  global word_encoding\n",
        "\n",
        "  words = text.lower().split(\" \")\n",
        "  encoding = []\n",
        "\n",
        "  for word in words:\n",
        "    if word in vocab:\n",
        "      code = vocab[word]\n",
        "      encoding.append(code)\n",
        "    else:\n",
        "      vocab[word] = word_encoding\n",
        "      encoding.append(word_encoding)\n",
        "      word_encoding += 1\n",
        "\n",
        "  return encoding\n",
        "\n",
        "text = \"this is a test to see if this test will work is is test a a\"\n",
        "encoding = one_hot_encoding(text)\n",
        "print(encoding)\n",
        "print(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IkNsMEyckS3h",
        "outputId": "765c1312-329d-4901-f714-77000cb3a85c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 2, 3, 4, 5, 6, 7, 1, 4, 8, 9, 2, 2, 4, 3, 3]\n",
            "{'this': 1, 'is': 2, 'a': 3, 'test': 4, 'to': 5, 'see': 6, 'if': 7, 'will': 8, 'work': 9}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "positive_review = \"I thought the movie was going to be bad but it was actually amazing\"\n",
        "negative_review = \"I thought the movie was going to be amazing but it was actually bad\"\n",
        "\n",
        "pos_encode = one_hot_encoding(positive_review)\n",
        "neg_encode = one_hot_encoding(negative_review)\n",
        "\n",
        "print(\"Positive:\", pos_encode)\n",
        "print(\"Negative:\", neg_encode)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zeSwMy_9ka0o",
        "outputId": "da0f39b5-6786-401b-903b-f7a9f95a4e75"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Positive: [10, 11, 12, 13, 14, 15, 5, 16, 17, 18, 19, 14, 20, 21]\n",
            "Negative: [10, 11, 12, 13, 14, 15, 5, 16, 21, 18, 19, 14, 20, 17]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Much better, now we are keeping track of the order of words and we can tell where each occurs. But this still has a few issues with it. Ideally, when we encode words, we would like similar words to have similar labels and different words to have very different labels. For example, the words \"happy\" and \"joyful\" should probably have very similar labels so we can determine that they are similar. While words like \"horrible\" and \"amazing\" should probably have very different labels. The method we looked at above won't be able to do something like this for us. This could mean that the model will have a very difficult time determining if two words are similar or not, which could result in some pretty drastic performance impacts."
      ],
      "metadata": {
        "id": "VUz_bAhAlIkW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Word Embeddings\n",
        "Luckily, there is a third method that is far superior, word embeddings. This method keeps the order of words intact as well as encodes similar words with very similar labels. It attempts to not only encode the frequency and order of words but the meaning of those words in the sentence. It encodes each word as a dense vector that represents its context in the sentence.\n",
        "Unlike the previous techniques, word embeddings are learned by looking at many different training examples. You can add what's called an embedding layer to the beginning of your model, and while your model trains, your embedding layer will learn the correct embeddings for words. You can also use pre-trained embedding layers."
      ],
      "metadata": {
        "id": "2-UNCVAylJqL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def word_encoding(text):\n",
        "    # Define a simple word embedding matrix\n",
        "    word_embeddings = {\n",
        "        \"I\": np.array([1, 0, 0]),\n",
        "        \"thought\": np.array([0, 1, 0]),\n",
        "        \"the\": np.array([0, 0, 1]),\n",
        "        \"movie\": np.array([1, 1, 0]),\n",
        "        \"was\": np.array([0, 1, 1]),\n",
        "        \"going\": np.array([1, 0, 1]),\n",
        "        \"to\": np.array([1, 1, 1]),\n",
        "        \"be\": np.array([0, 0, 0]),\n",
        "        \"bad\": np.array([-1, 0, 0]),\n",
        "        \"but\": np.array([0, -1, 0]),\n",
        "        \"it\": np.array([0, 0, -1]),\n",
        "        \"actually\": np.array([0, 0, 0]),\n",
        "        \"amazing\": np.array([1, 1, 1])\n",
        "    }\n",
        "\n",
        "    encoded_text = []\n",
        "    for word in text.lower().split():\n",
        "        if word in word_embeddings:\n",
        "            encoded_text.append(word_embeddings[word])\n",
        "        else:\n",
        "            encoded_text.append(np.zeros(3))  # Handle unknown words\n",
        "\n",
        "    return encoded_text"
      ],
      "metadata": {
        "id": "qKDzYGTOktwo"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "positive_review = \"I thought the movie was going to be bad but it was actually amazing\"\n",
        "negative_review = \"I thought the movie was going to be amazing but it was actually bad\"\n",
        "\n",
        "pos_encode = word_encoding(positive_review)\n",
        "neg_encode = word_encoding(negative_review)\n",
        "\n",
        "print(\"Positive:\", pos_encode)\n",
        "print(\"Negative:\", neg_encode)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8b4x9ZO9lRDq",
        "outputId": "25e0a127-dda2-482c-f04e-91306c77c6c7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Positive: [array([0., 0., 0.]), array([0, 1, 0]), array([0, 0, 1]), array([1, 1, 0]), array([0, 1, 1]), array([1, 0, 1]), array([1, 1, 1]), array([0, 0, 0]), array([-1,  0,  0]), array([ 0, -1,  0]), array([ 0,  0, -1]), array([0, 1, 1]), array([0, 0, 0]), array([1, 1, 1])]\n",
            "Negative: [array([0., 0., 0.]), array([0, 1, 0]), array([0, 0, 1]), array([1, 1, 0]), array([0, 1, 1]), array([1, 0, 1]), array([1, 1, 1]), array([0, 0, 0]), array([1, 1, 1]), array([ 0, -1,  0]), array([ 0,  0, -1]), array([0, 1, 1]), array([0, 0, 0]), array([-1,  0,  0])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KeMKoMLwlTBH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}