{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TTSqa93RfYiB",
        "outputId": "62a7b120-0621-48a4-cbfb-d3da75ded557"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n"
          ]
        }
      ],
      "source": [
        "%tensorflow_version 2.x  # this line is not required unless you are in a notebook\n",
        "from keras.preprocessing import sequence\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "di3getNnfdCh",
        "outputId": "c970a389-1910-4ead-80bc-d1c64f10be10"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1115394/1115394 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read, then decode for py2 compat.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "print ('Length of text: {} characters'.format(len(text)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "liH75eupfh4K",
        "outputId": "0df30826-e3ec-4eb4-bf15-56cd7a41c774"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 1115394 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a look at the first 250 characters in text\n",
        "print(text[:250])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i5yKYXD2ftGz",
        "outputId": "4d761237-a9bc-4f48-f54b-6118ef0b8f9d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = sorted(set(text))\n",
        "# Creating a mapping from unique characters to indices\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "def text_to_int(text):\n",
        "  return np.array([char2idx[c] for c in text])\n",
        "\n",
        "text_as_int = text_to_int(text)"
      ],
      "metadata": {
        "id": "53u-I23Pfv9v"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lets look at how part of our text is encoded\n",
        "print(\"Text:\", text[:13])\n",
        "print(\"Encoded:\", text_to_int(text[:13]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sKadURThfy9a",
        "outputId": "25944c83-cfe4-40c8-c8bd-0ccff8613e5e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: First Citizen\n",
            "Encoded: [18 47 56 57 58  1 15 47 58 47 64 43 52]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def int_to_text(ints):\n",
        "  try:\n",
        "    ints = ints.numpy()\n",
        "  except:\n",
        "    pass\n",
        "  return ''.join(idx2char[ints])\n",
        "\n",
        "print(int_to_text(text_as_int[:13]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZZHa2wqbf0r4",
        "outputId": "b517f769-fdc8-4181-80ae-3f4e7c1bf55f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 100  # length of sequence for a training example\n",
        "examples_per_epoch = len(text)//(seq_length+1)\n",
        "\n",
        "# Create training examples / targets\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)"
      ],
      "metadata": {
        "id": "10c12vD6f2rc"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)"
      ],
      "metadata": {
        "id": "ra9libz8f4WV"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_input_target(chunk):  # for the example: hello\n",
        "    input_text = chunk[:-1]  # hell\n",
        "    target_text = chunk[1:]  # ello\n",
        "    return input_text, target_text  # hell, ello\n",
        "\n",
        "dataset = sequences.map(split_input_target)  # we use map to apply the above function to every entry"
      ],
      "metadata": {
        "id": "YQjQ90fdf6Gg"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for x, y in dataset.take(2):\n",
        "  print(\"\\n\\nEXAMPLE\\n\")\n",
        "  print(\"INPUT\")\n",
        "  print(int_to_text(x))\n",
        "  print(\"\\nOUTPUT\")\n",
        "  print(int_to_text(y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z13wN1Vyf8hY",
        "outputId": "34c47304-32c7-4cf5-d8d8-be605d650b65"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "EXAMPLE\n",
            "\n",
            "INPUT\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You\n",
            "\n",
            "OUTPUT\n",
            "irst Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You \n",
            "\n",
            "\n",
            "EXAMPLE\n",
            "\n",
            "INPUT\n",
            "are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you \n",
            "\n",
            "OUTPUT\n",
            "re all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you k\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "VOCAB_SIZE = len(vocab)  # vocab is number of unique characters\n",
        "EMBEDDING_DIM = 256\n",
        "RNN_UNITS = 1024\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "data = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "metadata": {
        "id": "rFluSczrf-PK"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              batch_input_shape=[batch_size, None]),\n",
        "    tf.keras.layers.LSTM(rnn_units,\n",
        "                        return_sequences=True,\n",
        "                        stateful=True,\n",
        "                        recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model\n",
        "\n",
        "model = build_model(VOCAB_SIZE,EMBEDDING_DIM, RNN_UNITS, BATCH_SIZE)\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9VuCjTdjgLgS",
        "outputId": "55b60317-8c35-4d8f-8520-18a090081b8f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (64, None, 256)           16640     \n",
            "                                                                 \n",
            " lstm (LSTM)                 (64, None, 1024)          5246976   \n",
            "                                                                 \n",
            " dense (Dense)               (64, None, 65)            66625     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5330241 (20.33 MB)\n",
            "Trainable params: 5330241 (20.33 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example_batch, target_example_batch in data.take(1):\n",
        "  example_batch_predictions = model(input_example_batch)  # ask our model for a prediction on our first batch of training data (64 entries)\n",
        "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")  # print out the output shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RVSYGuhTgNZ-",
        "outputId": "f3ff1287-7a46-4780-b57a-481c6d1aa937"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 65) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we can see that the predicition is an array of 64 arrays, one for each entry in the batch\n",
        "print(len(example_batch_predictions))\n",
        "print(example_batch_predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jG7D8XUrgQKG",
        "outputId": "9441f217-ee13-4df7-c138-eb93ff251e28"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "64\n",
            "tf.Tensor(\n",
            "[[[-4.14056564e-03  1.08433235e-03  2.02269177e-03 ...  1.20480789e-03\n",
            "    1.52766914e-03 -5.67873335e-03]\n",
            "  [-4.31528222e-03 -7.91878672e-04 -7.19541777e-03 ... -5.74674131e-03\n",
            "    3.35317338e-03 -3.50675476e-03]\n",
            "  [-1.50293543e-03  3.53602972e-03  2.84383353e-03 ... -7.97469914e-03\n",
            "    3.97134863e-04 -8.99592601e-03]\n",
            "  ...\n",
            "  [ 2.76337331e-03  8.78215116e-03  1.04024522e-02 ... -1.50619063e-03\n",
            "    1.21639529e-02 -6.10082783e-03]\n",
            "  [ 4.76020761e-03  1.19636208e-02  6.74887095e-04 ...  6.36867248e-04\n",
            "    1.34632234e-02 -3.82454251e-03]\n",
            "  [ 2.36404059e-03  5.46256918e-03  3.35026096e-04 ...  9.52087459e-04\n",
            "    1.23441685e-02 -8.18961672e-03]]\n",
            "\n",
            " [[ 6.06970512e-04  2.39540217e-03  1.11724576e-03 ... -2.21652398e-03\n",
            "   -2.10186630e-03  3.92297469e-03]\n",
            "  [ 1.31679780e-03  4.53987578e-03  8.92355572e-04 ...  2.82555539e-03\n",
            "    1.08442083e-02  2.13491335e-03]\n",
            "  [ 8.74141522e-04  4.29054722e-03 -1.10707479e-03 ...  8.68508033e-03\n",
            "    7.37642404e-03  1.30316708e-04]\n",
            "  ...\n",
            "  [-6.85740728e-03 -1.51987830e-02 -2.01923698e-02 ...  5.30480128e-03\n",
            "   -1.49881211e-03 -2.62818974e-03]\n",
            "  [-5.74551336e-03 -9.87059809e-03 -1.04406318e-02 ...  4.45232773e-03\n",
            "   -4.37652040e-03 -1.44026999e-03]\n",
            "  [-4.21106676e-03 -4.58789710e-03 -7.03064678e-03 ...  8.35800730e-03\n",
            "    8.31455458e-03 -8.32629448e-04]]\n",
            "\n",
            " [[-2.21283454e-03 -2.43415358e-03 -4.46008611e-03 ... -3.73323960e-03\n",
            "   -1.41732744e-04 -2.66770762e-03]\n",
            "  [ 2.12537195e-03 -5.14154462e-03 -4.90598613e-04 ... -5.12419362e-03\n",
            "    6.69276225e-04 -5.31960651e-03]\n",
            "  [ 4.33272682e-04 -7.87425600e-03 -6.38157967e-03 ...  9.31089977e-04\n",
            "   -4.95485263e-04  3.09106125e-03]\n",
            "  ...\n",
            "  [ 2.37890333e-03  4.69037239e-03  4.25217301e-03 ... -1.11993318e-02\n",
            "   -4.25086357e-04 -3.28183966e-03]\n",
            "  [ 1.76557037e-03  4.16368246e-04  2.60514789e-03 ... -7.29815103e-03\n",
            "   -5.00400597e-03 -4.44809394e-03]\n",
            "  [ 1.44133531e-03  2.74377642e-03  2.33277958e-03 ... -7.65558286e-03\n",
            "   -5.05978102e-03  5.72688878e-05]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[ 4.56885528e-03  1.12564187e-03 -8.15198757e-04 ...  5.81055926e-03\n",
            "   -1.82556268e-03  1.46849966e-03]\n",
            "  [ 1.28652528e-03  4.35861386e-03  1.00548798e-03 ...  2.43332889e-03\n",
            "    5.83236397e-04  3.73679120e-03]\n",
            "  [ 5.36002452e-03  3.47235450e-03  3.46501544e-03 ...  8.18046182e-03\n",
            "   -8.48334283e-04  2.20197323e-03]\n",
            "  ...\n",
            "  [ 6.50832430e-03 -8.99153389e-03 -2.58814800e-03 ... -1.89195853e-04\n",
            "   -1.30758772e-03  3.64255207e-03]\n",
            "  [ 6.16455358e-03 -3.27996095e-03 -1.68629817e-03 ... -3.59589793e-03\n",
            "    1.42603181e-04  8.45906511e-03]\n",
            "  [ 5.68648102e-03 -5.26542775e-04  3.75764794e-04 ... -4.85195220e-03\n",
            "   -5.48150099e-04  9.21456423e-03]]\n",
            "\n",
            " [[ 1.98453083e-04 -2.91003427e-03  4.94876225e-03 ... -1.91442482e-03\n",
            "    4.62284870e-03 -3.57645750e-03]\n",
            "  [-9.71639995e-04 -4.98143677e-03  3.13134864e-03 ...  2.18273650e-04\n",
            "   -1.94060709e-03 -4.71112970e-03]\n",
            "  [-6.26956020e-03 -2.44143908e-03  3.91257321e-03 ...  1.59720425e-03\n",
            "    4.05590981e-04 -9.41319577e-03]\n",
            "  ...\n",
            "  [ 2.98049673e-03  3.50185833e-03 -7.11307302e-03 ... -2.53085094e-03\n",
            "    9.49303340e-03 -4.04229574e-03]\n",
            "  [ 4.76809032e-03  6.83460385e-03  2.03975127e-03 ... -6.40263548e-03\n",
            "    5.58158429e-03 -9.16550495e-03]\n",
            "  [-6.31992007e-04  6.39687851e-03  2.48829531e-03 ... -3.12034506e-03\n",
            "    6.09614979e-03 -1.28303291e-02]]\n",
            "\n",
            " [[ 3.86432512e-03  4.30030655e-03 -2.79227458e-03 ...  1.18376361e-03\n",
            "    5.22993039e-03  1.66808465e-03]\n",
            "  [ 1.17943832e-03  4.25398489e-03 -9.03749093e-03 ... -2.23516300e-03\n",
            "    4.30091005e-03  4.59034555e-03]\n",
            "  [ 1.94638013e-03  2.54496932e-04 -8.18693824e-03 ... -4.76947287e-03\n",
            "   -1.40215852e-03  8.60065594e-03]\n",
            "  ...\n",
            "  [ 3.48861702e-03 -6.00221055e-03  3.70337395e-03 ...  3.69423535e-04\n",
            "   -2.44906778e-03 -2.83586606e-03]\n",
            "  [-1.07429759e-03 -4.35233396e-03  5.75137977e-03 ...  4.76154964e-04\n",
            "   -7.85901211e-05 -8.62566009e-03]\n",
            "  [ 1.28200487e-03 -7.95035530e-03  3.83193255e-03 ... -3.31051415e-03\n",
            "   -4.16750228e-03 -2.67715845e-03]]], shape=(64, 100, 65), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# lets examine one prediction\n",
        "pred = example_batch_predictions[0]\n",
        "print(len(pred))\n",
        "print(pred)\n",
        "# notice this is a 2d array of length 100, where each interior array is the prediction for the next character at each time step"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ykxfaa3FgSKJ",
        "outputId": "4ddc2ca7-256b-4b82-fe42-30b085c7e9fc"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100\n",
            "tf.Tensor(\n",
            "[[-0.00414057  0.00108433  0.00202269 ...  0.00120481  0.00152767\n",
            "  -0.00567873]\n",
            " [-0.00431528 -0.00079188 -0.00719542 ... -0.00574674  0.00335317\n",
            "  -0.00350675]\n",
            " [-0.00150294  0.00353603  0.00284383 ... -0.0079747   0.00039713\n",
            "  -0.00899593]\n",
            " ...\n",
            " [ 0.00276337  0.00878215  0.01040245 ... -0.00150619  0.01216395\n",
            "  -0.00610083]\n",
            " [ 0.00476021  0.01196362  0.00067489 ...  0.00063687  0.01346322\n",
            "  -0.00382454]\n",
            " [ 0.00236404  0.00546257  0.00033503 ...  0.00095209  0.01234417\n",
            "  -0.00818962]], shape=(100, 65), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# and finally well look at a prediction at the first timestep\n",
        "time_pred = pred[0]\n",
        "print(len(time_pred))\n",
        "print(time_pred)\n",
        "# and of course its 65 values representing the probabillity of each character occuring next"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AAf0MmolgU3q",
        "outputId": "cea89c92-0bb3-4e3e-b78a-daea6454056e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65\n",
            "tf.Tensor(\n",
            "[-4.1405656e-03  1.0843324e-03  2.0226918e-03  1.0953355e-03\n",
            "  3.0071463e-03 -2.3158445e-04  6.1822450e-03  1.1370194e-03\n",
            " -4.6029389e-03 -4.3508261e-03 -7.6271559e-04 -1.8469797e-03\n",
            " -3.4441021e-03 -9.0333592e-04  2.6899781e-03  6.7069079e-05\n",
            "  7.0362585e-05 -1.9395959e-03 -7.2775036e-04  2.2960538e-03\n",
            " -2.0424107e-03  5.5257371e-04 -2.6483568e-03 -4.5378576e-05\n",
            "  9.5729937e-04 -4.5932196e-03  4.5129107e-03  1.6334774e-03\n",
            "  2.5232555e-05 -4.7260909e-03 -4.7311340e-03  6.7850109e-03\n",
            " -5.9521543e-03  3.6394615e-03  4.2462457e-04 -2.0341312e-03\n",
            "  1.6451036e-03  2.1803160e-03 -4.8535992e-04 -4.3144077e-03\n",
            " -2.8610763e-03  2.4315470e-04  5.3949503e-04 -2.5805621e-04\n",
            " -1.3344134e-03 -2.1602651e-03  1.1488923e-03 -2.1012931e-03\n",
            "  1.5409433e-03 -4.7679464e-03 -3.7406192e-03  1.5120098e-03\n",
            " -7.1631400e-03  1.9905784e-03  1.5653864e-03  5.5053266e-04\n",
            " -1.8605326e-03 -5.0201640e-03 -2.1788559e-03  2.4241309e-03\n",
            "  3.3278117e-04 -7.4643269e-04  1.2048079e-03  1.5276691e-03\n",
            " -5.6787333e-03], shape=(65,), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# If we want to determine the predicted character we need to sample the output distribution (pick a value based on probabillity)\n",
        "sampled_indices = tf.random.categorical(pred, num_samples=1)\n",
        "\n",
        "# now we can reshape that array and convert all the integers to numbers to see the actual characters\n",
        "sampled_indices = np.reshape(sampled_indices, (1, -1))[0]\n",
        "predicted_chars = int_to_text(sampled_indices)\n",
        "\n",
        "predicted_chars  # and this is what the model predicted for training sequence 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "QWgtHOOlgWwv",
        "outputId": "235b7dfc-0e7a-4e41-ca46-ea1b43a177a0"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\".PHFq'JL$R3H,aVq$wQ-Oz&E,frsdkZrDGeSrVJWWYtkn&UcP\\n!\\nrCCclZWcbpNugmT3Ncpl3Jdz'iwNbVH3rJBC KhC:;ocws!X\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
      ],
      "metadata": {
        "id": "hu1pT1G5gYow"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "metadata": {
        "id": "z0YN_Upngaln"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "metadata": {
        "id": "ehEnugkVgcJc"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(data, epochs=50, callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gtl6ZeLNgeCy",
        "outputId": "d59643bd-f342-4867-f86f-16e4e76f81ad"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "172/172 [==============================] - 17s 65ms/step - loss: 2.5989\n",
            "Epoch 2/50\n",
            "172/172 [==============================] - 12s 62ms/step - loss: 1.8901\n",
            "Epoch 3/50\n",
            "172/172 [==============================] - 12s 63ms/step - loss: 1.6444\n",
            "Epoch 4/50\n",
            "172/172 [==============================] - 14s 68ms/step - loss: 1.5140\n",
            "Epoch 5/50\n",
            "172/172 [==============================] - 13s 66ms/step - loss: 1.4335\n",
            "Epoch 6/50\n",
            "172/172 [==============================] - 13s 66ms/step - loss: 1.3771\n",
            "Epoch 7/50\n",
            "172/172 [==============================] - 13s 66ms/step - loss: 1.3328\n",
            "Epoch 8/50\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 1.2938\n",
            "Epoch 9/50\n",
            "172/172 [==============================] - 13s 69ms/step - loss: 1.2612\n",
            "Epoch 10/50\n",
            "172/172 [==============================] - 13s 69ms/step - loss: 1.2276\n",
            "Epoch 11/50\n",
            "172/172 [==============================] - 14s 71ms/step - loss: 1.1949\n",
            "Epoch 12/50\n",
            "172/172 [==============================] - 14s 71ms/step - loss: 1.1622\n",
            "Epoch 13/50\n",
            "172/172 [==============================] - 14s 72ms/step - loss: 1.1288\n",
            "Epoch 14/50\n",
            "172/172 [==============================] - 14s 73ms/step - loss: 1.0942\n",
            "Epoch 15/50\n",
            "172/172 [==============================] - 15s 73ms/step - loss: 1.0568\n",
            "Epoch 16/50\n",
            "172/172 [==============================] - 14s 72ms/step - loss: 1.0198\n",
            "Epoch 17/50\n",
            "172/172 [==============================] - 14s 74ms/step - loss: 0.9807\n",
            "Epoch 18/50\n",
            "172/172 [==============================] - 14s 72ms/step - loss: 0.9426\n",
            "Epoch 19/50\n",
            "172/172 [==============================] - 14s 73ms/step - loss: 0.9039\n",
            "Epoch 20/50\n",
            "172/172 [==============================] - 14s 74ms/step - loss: 0.8664\n",
            "Epoch 21/50\n",
            "172/172 [==============================] - 14s 73ms/step - loss: 0.8315\n",
            "Epoch 22/50\n",
            "172/172 [==============================] - 14s 71ms/step - loss: 0.7957\n",
            "Epoch 23/50\n",
            "172/172 [==============================] - 14s 73ms/step - loss: 0.7633\n",
            "Epoch 24/50\n",
            "172/172 [==============================] - 15s 74ms/step - loss: 0.7330\n",
            "Epoch 25/50\n",
            "172/172 [==============================] - 15s 73ms/step - loss: 0.7030\n",
            "Epoch 26/50\n",
            "172/172 [==============================] - 14s 71ms/step - loss: 0.6783\n",
            "Epoch 27/50\n",
            "172/172 [==============================] - 15s 74ms/step - loss: 0.6544\n",
            "Epoch 28/50\n",
            "172/172 [==============================] - 14s 73ms/step - loss: 0.6321\n",
            "Epoch 29/50\n",
            "172/172 [==============================] - 14s 73ms/step - loss: 0.6124\n",
            "Epoch 30/50\n",
            "172/172 [==============================] - 14s 71ms/step - loss: 0.5925\n",
            "Epoch 31/50\n",
            "172/172 [==============================] - 14s 73ms/step - loss: 0.5760\n",
            "Epoch 32/50\n",
            "172/172 [==============================] - 14s 74ms/step - loss: 0.5626\n",
            "Epoch 33/50\n",
            "172/172 [==============================] - 14s 72ms/step - loss: 0.5484\n",
            "Epoch 34/50\n",
            "172/172 [==============================] - 14s 72ms/step - loss: 0.5367\n",
            "Epoch 35/50\n",
            "172/172 [==============================] - 14s 72ms/step - loss: 0.5251\n",
            "Epoch 36/50\n",
            "172/172 [==============================] - 15s 74ms/step - loss: 0.5164\n",
            "Epoch 37/50\n",
            "172/172 [==============================] - 14s 73ms/step - loss: 0.5051\n",
            "Epoch 38/50\n",
            "172/172 [==============================] - 14s 72ms/step - loss: 0.4964\n",
            "Epoch 39/50\n",
            "172/172 [==============================] - 14s 72ms/step - loss: 0.4907\n",
            "Epoch 40/50\n",
            "172/172 [==============================] - 14s 72ms/step - loss: 0.4819\n",
            "Epoch 41/50\n",
            "172/172 [==============================] - 14s 72ms/step - loss: 0.4747\n",
            "Epoch 42/50\n",
            "172/172 [==============================] - 14s 73ms/step - loss: 0.4689\n",
            "Epoch 43/50\n",
            "172/172 [==============================] - 14s 73ms/step - loss: 0.4643\n",
            "Epoch 44/50\n",
            "172/172 [==============================] - 14s 72ms/step - loss: 0.4596\n",
            "Epoch 45/50\n",
            "172/172 [==============================] - 15s 71ms/step - loss: 0.4547\n",
            "Epoch 46/50\n",
            "172/172 [==============================] - 14s 72ms/step - loss: 0.4508\n",
            "Epoch 47/50\n",
            "172/172 [==============================] - 15s 72ms/step - loss: 0.4463\n",
            "Epoch 48/50\n",
            "172/172 [==============================] - 14s 75ms/step - loss: 0.4409\n",
            "Epoch 49/50\n",
            "172/172 [==============================] - 14s 73ms/step - loss: 0.4377\n",
            "Epoch 50/50\n",
            "172/172 [==============================] - 14s 72ms/step - loss: 0.4347\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, batch_size=1)"
      ],
      "metadata": {
        "id": "XrktOBV8guDl"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "metadata": {
        "id": "gnSFEuHgg5Ej"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, start_string):\n",
        "  # Evaluation step (generating text using the learned model)\n",
        "\n",
        "  # Number of characters to generate\n",
        "  num_generate = 800\n",
        "\n",
        "  # Converting our start string to numbers (vectorizing)\n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # Empty string to store our results\n",
        "  text_generated = []\n",
        "\n",
        "  # Low temperatures results in more predictable text.\n",
        "  # Higher temperatures results in more surprising text.\n",
        "  # Experiment to find the best setting.\n",
        "  temperature = 1.0\n",
        "\n",
        "  # Here batch size == 1\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "      predictions = model(input_eval)\n",
        "      # remove the batch dimension\n",
        "\n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "      # using a categorical distribution to predict the character returned by the model\n",
        "      predictions = predictions / temperature\n",
        "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "      # We pass the predicted character as the next input to the model\n",
        "      # along with the previous hidden state\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "      text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))"
      ],
      "metadata": {
        "id": "gSMg0hnrg8v4"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inp = input(\"Type a starting string: \")\n",
        "print(generate_text(model, inp))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9XqoSJm0g-11",
        "outputId": "221b661d-e074-4998-9586-515ac2e9bd64"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Type a starting string: sun\n",
            "sung frame of Coriolanus music grave\n",
            "As e'er my mistress, one hath pattern gates i' the tows\n",
            "To find the all he fell a lift\n",
            "Of mortal tred therefore?\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "I know not where before thou diest to-morrow.\n",
            "\n",
            "LORD FITZWATER:\n",
            "How fares the prince?\n",
            "\n",
            "Messenger:\n",
            "The will shall be his coming hither to command\n",
            "At up, a beggar bennd colouring;\n",
            "back, that he is band for loving praise\n",
            "Live in thy touch, whose coward as to-joint widows,\n",
            "Dismiss the THIO:\n",
            "Spiece! we'll die.\n",
            "\n",
            "ISABELLA:\n",
            "I do, my lord; your brother, tell me, hated the corse\n",
            "That joy had worn it out.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "Sended to the rotten years another rock,\n",
            "That bounds be a brief.\n",
            "\n",
            "PAGE:\n",
            "There is no matter; I will lie with thee here:\n",
            "O, that a man entake his late kingdom fear'd\n",
            "And frantis mean to tast when I parter.\n",
            "\n",
            "GLOUCESTER:\n",
            "Have \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0L3WtpOWjzqC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}